{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\py3.11\\Lib\\site-packages\\torchvision\\datasets\\mnist.py:81: UserWarning: test_data has been renamed data\n",
      "  warnings.warn(\"test_data has been renamed data\")\n",
      "C:\\Users\\19914\\AppData\\Local\\Temp\\ipykernel_35460\\1784665630.py:24: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  test_x = Variable(torch.unsqueeze(test_data.test_data,dim  = 1),volatile = True).type(torch.FloatTensor)[:500]/255.\n",
      "D:\\py3.11\\Lib\\site-packages\\torchvision\\datasets\\mnist.py:71: UserWarning: test_labels has been renamed targets\n",
      "  warnings.warn(\"test_labels has been renamed targets\")\n",
      "C:\\Users\\19914\\AppData\\Local\\Temp\\ipykernel_35460\\1784665630.py:74: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  output = F.softmax(out2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== 20 ===== ===== test accuracy is  0.194 ==========\n",
      "========== 40 ===== ===== test accuracy is  0.344 ==========\n",
      "========== 60 ===== ===== test accuracy is  0.44 ==========\n",
      "========== 80 ===== ===== test accuracy is  0.578 ==========\n",
      "========== 100 ===== ===== test accuracy is  0.612 ==========\n",
      "========== 120 ===== ===== test accuracy is  0.678 ==========\n",
      "========== 140 ===== ===== test accuracy is  0.696 ==========\n",
      "========== 160 ===== ===== test accuracy is  0.738 ==========\n",
      "========== 180 ===== ===== test accuracy is  0.78 ==========\n",
      "========== 200 ===== ===== test accuracy is  0.804 ==========\n",
      "========== 220 ===== ===== test accuracy is  0.822 ==========\n",
      "========== 240 ===== ===== test accuracy is  0.832 ==========\n",
      "========== 260 ===== ===== test accuracy is  0.824 ==========\n",
      "========== 280 ===== ===== test accuracy is  0.856 ==========\n",
      "========== 300 ===== ===== test accuracy is  0.864 ==========\n",
      "========== 320 ===== ===== test accuracy is  0.858 ==========\n",
      "========== 340 ===== ===== test accuracy is  0.868 ==========\n",
      "========== 360 ===== ===== test accuracy is  0.876 ==========\n",
      "========== 380 ===== ===== test accuracy is  0.884 ==========\n",
      "========== 400 ===== ===== test accuracy is  0.898 ==========\n",
      "========== 420 ===== ===== test accuracy is  0.902 ==========\n",
      "========== 440 ===== ===== test accuracy is  0.908 ==========\n",
      "========== 460 ===== ===== test accuracy is  0.908 ==========\n",
      "========== 480 ===== ===== test accuracy is  0.902 ==========\n",
      "========== 500 ===== ===== test accuracy is  0.904 ==========\n",
      "========== 520 ===== ===== test accuracy is  0.884 ==========\n",
      "========== 540 ===== ===== test accuracy is  0.912 ==========\n",
      "========== 560 ===== ===== test accuracy is  0.916 ==========\n",
      "========== 580 ===== ===== test accuracy is  0.908 ==========\n",
      "========== 600 ===== ===== test accuracy is  0.91 ==========\n",
      "========== 620 ===== ===== test accuracy is  0.914 ==========\n",
      "========== 640 ===== ===== test accuracy is  0.92 ==========\n",
      "========== 660 ===== ===== test accuracy is  0.916 ==========\n",
      "========== 680 ===== ===== test accuracy is  0.926 ==========\n",
      "========== 700 ===== ===== test accuracy is  0.93 ==========\n",
      "========== 720 ===== ===== test accuracy is  0.924 ==========\n",
      "========== 740 ===== ===== test accuracy is  0.928 ==========\n",
      "========== 760 ===== ===== test accuracy is  0.926 ==========\n",
      "========== 780 ===== ===== test accuracy is  0.93 ==========\n",
      "========== 800 ===== ===== test accuracy is  0.928 ==========\n",
      "========== 820 ===== ===== test accuracy is  0.934 ==========\n",
      "========== 840 ===== ===== test accuracy is  0.944 ==========\n",
      "========== 860 ===== ===== test accuracy is  0.928 ==========\n",
      "========== 880 ===== ===== test accuracy is  0.936 ==========\n",
      "========== 900 ===== ===== test accuracy is  0.916 ==========\n",
      "========== 920 ===== ===== test accuracy is  0.956 ==========\n",
      "========== 940 ===== ===== test accuracy is  0.944 ==========\n",
      "========== 960 ===== ===== test accuracy is  0.94 ==========\n",
      "========== 980 ===== ===== test accuracy is  0.934 ==========\n",
      "========== 1000 ===== ===== test accuracy is  0.938 ==========\n",
      "========== 1020 ===== ===== test accuracy is  0.942 ==========\n",
      "========== 1040 ===== ===== test accuracy is  0.95 ==========\n",
      "========== 1060 ===== ===== test accuracy is  0.94 ==========\n",
      "========== 1080 ===== ===== test accuracy is  0.942 ==========\n",
      "========== 1100 ===== ===== test accuracy is  0.946 ==========\n",
      "========== 1120 ===== ===== test accuracy is  0.956 ==========\n",
      "========== 1140 ===== ===== test accuracy is  0.956 ==========\n",
      "========== 1160 ===== ===== test accuracy is  0.966 ==========\n",
      "========== 1180 ===== ===== test accuracy is  0.94 ==========\n",
      "========== 20 ===== ===== test accuracy is  0.952 ==========\n",
      "========== 40 ===== ===== test accuracy is  0.946 ==========\n",
      "========== 60 ===== ===== test accuracy is  0.942 ==========\n",
      "========== 80 ===== ===== test accuracy is  0.952 ==========\n",
      "========== 100 ===== ===== test accuracy is  0.954 ==========\n",
      "========== 120 ===== ===== test accuracy is  0.954 ==========\n",
      "========== 140 ===== ===== test accuracy is  0.96 ==========\n",
      "========== 160 ===== ===== test accuracy is  0.948 ==========\n",
      "========== 180 ===== ===== test accuracy is  0.952 ==========\n",
      "========== 200 ===== ===== test accuracy is  0.946 ==========\n",
      "========== 220 ===== ===== test accuracy is  0.95 ==========\n",
      "========== 240 ===== ===== test accuracy is  0.962 ==========\n",
      "========== 260 ===== ===== test accuracy is  0.952 ==========\n",
      "========== 280 ===== ===== test accuracy is  0.96 ==========\n",
      "========== 300 ===== ===== test accuracy is  0.962 ==========\n",
      "========== 320 ===== ===== test accuracy is  0.962 ==========\n",
      "========== 340 ===== ===== test accuracy is  0.956 ==========\n",
      "========== 360 ===== ===== test accuracy is  0.958 ==========\n",
      "========== 380 ===== ===== test accuracy is  0.962 ==========\n",
      "========== 400 ===== ===== test accuracy is  0.964 ==========\n",
      "========== 420 ===== ===== test accuracy is  0.95 ==========\n",
      "========== 440 ===== ===== test accuracy is  0.964 ==========\n",
      "========== 460 ===== ===== test accuracy is  0.966 ==========\n",
      "========== 480 ===== ===== test accuracy is  0.97 ==========\n",
      "========== 500 ===== ===== test accuracy is  0.958 ==========\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 105\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m'\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m    104\u001b[39m     cnn = CNN()\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcnn\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 98\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(cnn)\u001b[39m\n\u001b[32m     96\u001b[39m optimizer.zero_grad()\n\u001b[32m     97\u001b[39m loss.backward()\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m step != \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m step % \u001b[32m20\u001b[39m ==\u001b[32m0\u001b[39m:\n\u001b[32m    101\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m10\u001b[39m,step,\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m5\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m5\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtest accuracy is \u001b[39m\u001b[33m\"\u001b[39m,test(cnn) ,\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m10\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\py3.11\\Lib\\site-packages\\torch\\optim\\optimizer.py:487\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    482\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    483\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    484\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    485\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m487\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    490\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\py3.11\\Lib\\site-packages\\torch\\optim\\optimizer.py:91\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     89\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     90\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     93\u001b[39m     torch._dynamo.graph_break()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\py3.11\\Lib\\site-packages\\torch\\optim\\adam.py:223\u001b[39m, in \u001b[36mAdam.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    211\u001b[39m     beta1, beta2 = group[\u001b[33m\"\u001b[39m\u001b[33mbetas\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    213\u001b[39m     has_complex = \u001b[38;5;28mself\u001b[39m._init_group(\n\u001b[32m    214\u001b[39m         group,\n\u001b[32m    215\u001b[39m         params_with_grad,\n\u001b[32m   (...)\u001b[39m\u001b[32m    220\u001b[39m         state_steps,\n\u001b[32m    221\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m223\u001b[39m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    224\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    226\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    227\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    228\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    229\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    230\u001b[39m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mamsgrad\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    231\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    232\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    233\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    234\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    235\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    236\u001b[39m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaximize\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mforeach\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcapturable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    240\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdifferentiable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    241\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfused\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    242\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgrad_scale\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    243\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfound_inf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    244\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    246\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\py3.11\\Lib\\site-packages\\torch\\optim\\optimizer.py:154\u001b[39m, in \u001b[36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    152\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(*args, **kwargs)\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\py3.11\\Lib\\site-packages\\torch\\optim\\adam.py:784\u001b[39m, in \u001b[36madam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[39m\n\u001b[32m    781\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    782\u001b[39m     func = _single_tensor_adam\n\u001b[32m--> \u001b[39m\u001b[32m784\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    785\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    786\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    787\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    801\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    803\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\py3.11\\Lib\\site-packages\\torch\\optim\\adam.py:432\u001b[39m, in \u001b[36m_single_tensor_adam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[39m\n\u001b[32m    429\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    430\u001b[39m         denom = (exp_avg_sq.sqrt() / bias_correction2_sqrt).add_(eps)\n\u001b[32m--> \u001b[39m\u001b[32m432\u001b[39m     \u001b[43mparam\u001b[49m\u001b[43m.\u001b[49m\u001b[43maddcdiv_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexp_avg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdenom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[43mstep_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    434\u001b[39m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n\u001b[32m    435\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m amsgrad \u001b[38;5;129;01mand\u001b[39;00m torch.is_complex(params[i]):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as Data\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "learning_rate = 1e-4\n",
    "keep_prob_rate = 0.7 #\n",
    "max_epoch = 3\n",
    "BATCH_SIZE = 50\n",
    "\n",
    "DOWNLOAD_MNIST = False\n",
    "if not(os.path.exists('./mnist/')) or not os.listdir('./mnist/'):\n",
    "    # not mnist dir or mnist is empyt dir\n",
    "    DOWNLOAD_MNIST = True\n",
    "\n",
    "\n",
    "train_data = torchvision.datasets.MNIST(root='./mnist/',train=True, transform=torchvision.transforms.ToTensor(), download=DOWNLOAD_MNIST,)\n",
    "train_loader = Data.DataLoader(dataset = train_data ,batch_size= BATCH_SIZE ,shuffle= True)\n",
    "\n",
    "test_data = torchvision.datasets.MNIST(root = './mnist/',train = False)\n",
    "test_x = Variable(torch.unsqueeze(test_data.test_data,dim  = 1),volatile = True).type(torch.FloatTensor)[:500]/255.\n",
    "test_y = test_data.test_labels[:500].numpy()\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d( # ???\n",
    "                # patch 7 * 7 ; 1  in channels ; 32 out channels ; ; stride is 1\n",
    "                # padding style is same(that means the convolution opration's input and output have the same size)\n",
    "                in_channels=  1    ,  \n",
    "                out_channels= 32    ,\n",
    "                kernel_size=   7   ,\n",
    "                stride=    1       ,\n",
    "                padding=  3        ,\n",
    "            ),\n",
    "            nn.ReLU(),        # activation function\n",
    "            nn.MaxPool2d(2),  # pooling operation\n",
    "        )\n",
    "        self.conv2 = nn.Sequential( # ???\n",
    "            # line 1 : convolution function, patch 5*5 , 32 in channels ;64 out channels; padding style is same; stride is 1\n",
    "            # line 2 : choosing your activation funciont\n",
    "            # line 3 : pooling operation function.\n",
    "             nn.Conv2d(\n",
    "                in_channels=32,       # 输入通道数 = 32\n",
    "                out_channels=64,      # 输出通道数 = 64\n",
    "                kernel_size=5,        # 卷积核大小 = 5x5\n",
    "                stride=1,             # 步幅 = 1\n",
    "                padding=2             # 保持尺寸不变 => padding = (5 - 1) / 2 = 2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)  # 最大池化，窗口大小 = 2x2，步幅 = 2\n",
    "         \n",
    "\n",
    "        )\n",
    "        self.out1 = nn.Linear( 7*7*64 , 1024 , bias= True)   # full connection layer one\n",
    "\n",
    "        self.dropout = nn.Dropout(keep_prob_rate)\n",
    "        self.out2 = nn.Linear(1024,10,bias=True)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = x.view( x.size(0), -1 )  # flatten the output of coonv2 to (batch_size ,32 * 7 * 7)    # ???\n",
    "        out1 = self.out1(x)\n",
    "        out1 = F.relu(out1)\n",
    "        out1 = self.dropout(out1)\n",
    "        out2 = self.out2(out1)\n",
    "        output = F.softmax(out2)\n",
    "        return output\n",
    "\n",
    "\n",
    "def test(cnn):\n",
    "    global prediction\n",
    "    y_pre = cnn(test_x)\n",
    "    _,pre_index= torch.max(y_pre,1)\n",
    "    pre_index= pre_index.view(-1)\n",
    "    prediction = pre_index.data.numpy()\n",
    "    correct  = np.sum(prediction == test_y)\n",
    "    return correct / 500.0\n",
    "\n",
    "\n",
    "def train(cnn):\n",
    "    optimizer = torch.optim.Adam(cnn.parameters(), lr=learning_rate )\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    for epoch in range(max_epoch):\n",
    "        for step, (x_, y_) in enumerate(train_loader):\n",
    "            x ,y= Variable(x_),Variable(y_)\n",
    "            output = cnn(x)  \n",
    "            loss = loss_func(output,y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if step != 0 and step % 20 ==0:\n",
    "                print(\"=\" * 10,step,\"=\"*5,\"=\"*5, \"test accuracy is \",test(cnn) ,\"=\" * 10 )\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    cnn = CNN()\n",
    "    train(cnn)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
